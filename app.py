# app.py

import os
from dotenv import load_dotenv
import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Ayarlar
DB_PATH = "./chroma_db_banka_sss"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# LLM_MODEL'Ä± gemini-pro yerine, gemini-2.5-flash veya gemini-pro olarak kullanÄ±n.
LLM_MODEL = "gemini-2.5-flash" 

@st.cache_resource
def load_rag_components():
    """RAG bileÅŸenlerini yÃ¼kler"""
    
    # âš ï¸ EK KONTROL: VeritabanÄ± varlÄ±ÄŸÄ±nÄ± kontrol et
    if not os.path.exists(DB_PATH):
        st.error(f"âŒ Hata: VektÃ¶r veritabanÄ± klasÃ¶rÃ¼ ({DB_PATH}) bulunamadÄ±.")
        st.error("LÃ¼tfen Ã¶nce 'python build_vector_db.py' komutunu Ã§alÄ±ÅŸtÄ±rarak veritabanÄ±nÄ± oluÅŸturun.")
        st.stop()
        
    st.info("ğŸ§  RAG bileÅŸenleri yÃ¼kleniyor...")
    
    # Embedding modeli
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME, 
        model_kwargs={'device': 'cpu'}
    )
    
    # VektÃ¶r veritabanÄ±
    vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # LLM Modeli AdÄ± GÃœNCELLENDÄ° (Hata veren ad yerine stabil ad kullanÄ±ldÄ±)
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2)
    
    st.success("âœ… RAG BileÅŸenleri BaÅŸarÄ±yla YÃ¼klendi.")
    return retriever, llm

# Prompt template (AynÄ± kaldÄ±)
template = """Sen, bir bankanÄ±n mÃ¼ÅŸteri hizmetleri temsilcisisin. Sadece aÅŸaÄŸÄ±da verilen 'BaÄŸlam' iÃ§indeki bilgilere dayanarak soruyu yanÄ±tla. BaÄŸlamda cevabÄ± bulunmayan bir soru sorulursa, "Bu konuda bir bilgim bulunmuyor, lÃ¼tfen bankanÄ±zla doÄŸrudan iletiÅŸime geÃ§in." de.

BaÄŸlam:
{context}

KullanÄ±cÄ± Sorusu: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

def create_rag_chain(retriever, llm):
    """RAG zinciri oluÅŸturur"""
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# Streamlit uygulamasÄ±
st.set_page_config(page_title="Akbank Sanal Asistan", layout="wide")
st.title("ğŸ¦ Akbank Sanal Asistan (RAG Chatbot)")
st.caption("GenAI Bootcamp Projesi | Finansal SSS yanÄ±tlayÄ±cÄ±sÄ±")

# RAG bileÅŸenlerini yÃ¼kle
retriever, llm = load_rag_components()
rag_chain = create_rag_chain(retriever, llm)

# Sohbet geÃ§miÅŸi (AynÄ± kaldÄ±)
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "Merhaba! Ben Akbank Sanal Asistan. Size finans ve bankacÄ±lÄ±k konularÄ±nda nasÄ±l yardÄ±mcÄ± olabilirim?"
    })

# Ã–nceki mesajlarÄ± gÃ¶ster (AynÄ± kaldÄ±)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ± giriÅŸi (AynÄ± kaldÄ±)
if prompt := st.chat_input("LÃ¼tfen sorunuzu buraya yazÄ±n..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± kaydet ve gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Chatbot yanÄ±tÄ±nÄ± Ã¼ret
    with st.chat_message("assistant"):
        with st.spinner("Cevap aranÄ±yor..."):
            try:
                response = rag_chain.invoke(prompt)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                # Hata durumunda kullanÄ±cÄ±ya bilgi ver
                error_msg = f"ÃœzgÃ¼nÃ¼m, Gemini API'den yanÄ±t alÄ±nÄ±rken bir hata oluÅŸtu. Hata detaylarÄ±: {e}"
                st.markdown(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

# Sidebar (AynÄ± kaldÄ±)
st.sidebar.title("Product KÄ±lavuzu")
st.sidebar.markdown("""
**Test SenaryolarÄ±:**
- **Kredi kartÄ± baÅŸvurusu nasÄ±l yapÄ±lÄ±r?**
- **DÃ¶viz hesabÄ± aÃ§mak iÃ§in ne gerekiyor?**
- **Hesap iÅŸletim Ã¼creti alÄ±yor musunuz?**
- **DÃ¼nyanÄ±n en yÃ¼ksek daÄŸÄ± nedir?** (bilgim yok testi)
""")